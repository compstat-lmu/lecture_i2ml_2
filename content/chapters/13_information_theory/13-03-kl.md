---
title: "Chapter 13.3: Kullback-Leibler Divergence"
---
The Kullback-Leibler divergence (KL) is an important quantity for measuring the difference between two probability distribution. We discuss different intuitions for KL and relate it to risk minimization and likelihood ratios. 

<!--more-->

### Lecture video

{{< video id="kC0XXQgC4_k" >}}

### Lecture slides

{{< pdfjs file="slides-info-kl.pdf" >}}
