<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Introduction to Machine Learning (I2ML)</title>
    <link>https://compstat-lmu.github.io/lecture_i2ml_2/</link>
    <description>Recent content on Introduction to Machine Learning (I2ML)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://compstat-lmu.github.io/lecture_i2ml_2/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/team/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/team/</guid>
      <description>Team  Bernd Bischl wrote the initial version of most of the course material, and teaches various master courses in ML and DL at the LMU for stats and data science. Ludwig Bothmann joined the team in summer 2020 and is in charge of the class held at LMU Munich. Fabian Scheipl joined the team in fall 2018 and contributed to the slides, videos and code demos. Heidi Seibold joined the team in fall 2019 and is in charge of the classes held in spring 2020 at LMU Munich and University of Bielefeld.</description>
    </item>
    
    <item>
      <title>Chapter 1.1: What is ML?</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/01_ml_basics/01-01-basics-whatisml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/01_ml_basics/01-01-basics-whatisml/</guid>
      <description>&lt;p&gt;As subtopic of artificial intelligence, machine learning is a mathematically well-defined discipline and usually constructs predictive or decision models from data, instead of explicitly programming them. In this section, you will see some typical examples of where machine learning is applied and the main directions of machine learning.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 1.2: Data</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/01_ml_basics/01-02-data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/01_ml_basics/01-02-data/</guid>
      <description>&lt;p&gt;In this section we explain the basic structure of tabular data used in machine learning. We will discern targets from features, talk about labeled and unlabeled data and introduce the concept of the data-generating process.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 1.3: Tasks</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/01_ml_basics/01-03-tasks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/01_ml_basics/01-03-tasks/</guid>
      <description>&lt;p&gt;The tasks of supervised learning can roughly be divided in two categories: regression (for continuous outcome) and classification (for categorical outcome). We will present some examples.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 1.4: Models and Parameters</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/01_ml_basics/01-04-models-parameters/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/01_ml_basics/01-04-models-parameters/</guid>
      <description>&lt;p&gt;We introduce models as functional hypotheses about the mapping from feature to target space that allow us to make predictions by computing a function of the input data. Frequently in machine learning, models are understood to be parameterized curves, which is illustrated by several examples.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 1.5: Learner</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/01_ml_basics/01-05-learner/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/01_ml_basics/01-05-learner/</guid>
      <description>&lt;p&gt;Roughly speaking, learners (endowed with a specific hyperparameter configuration) take training data and return a model.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 1.6: Losses and Risk Minimization</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/01_ml_basics/01-06-riskminimization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/01_ml_basics/01-06-riskminimization/</guid>
      <description>&lt;p&gt;In order to find good models we need a concept to evaluate and compare models. To this end, the concepts of &lt;em&gt;loss function&lt;/em&gt;, &lt;em&gt;risk&lt;/em&gt; and &lt;em&gt;empirical risk minimization&lt;/em&gt; are introduced.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 1.7: Optimization</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/01_ml_basics/01-07-optimization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/01_ml_basics/01-07-optimization/</guid>
      <description>&lt;p&gt;In this section we study parameter optimization as computational solution to machine learning problems. We address pitfalls in non-convex optimization problems and introduce the fundamental concept of gradient descent.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 1.8: Components of a Learner</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/01_ml_basics/01-08-learnercomponents-hro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/01_ml_basics/01-08-learnercomponents-hro/</guid>
      <description>&lt;p&gt;Nearly all supervised learning algorithms can be described in terms of three components: 1) hypothesis space, 2) risk, and 3) optimization. In this section, we explain how these components interact and why this is a very useful concept for many supervised learning approaches.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 2.1: Loss Functions for Regression</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/02_supervised_regression/02-01-losses/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/02_supervised_regression/02-01-losses/</guid>
      <description>&lt;p&gt;\(L1\) and \(L2\) are two essential loss functions used for evaluating the performance of regression models. This section defines \(L1\) and \(L2\) loss and explains the differences.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 2.2: Linear Regression Models</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/02_supervised_regression/02-02-linearmodel/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/02_supervised_regression/02-02-linearmodel/</guid>
      <description>&lt;p&gt;In this section, we explain how the linear regression model can be used from a machine learning perspective to predict a continuous numerical target variable. We use the concepts of loss function and empirical risk minimization to find the linear model that best fits the data.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 2.3: Polynomial Regression Models</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/02_supervised_regression/02-03-polynomials/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/02_supervised_regression/02-03-polynomials/</guid>
      <description>&lt;p&gt;This section introduces polynomials to obtain more flexible models for the regression task. We explain the connection to the basic linear model and discuss the problem of overfitting.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 3.1: Classification Tasks</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/03_supervised_classification/03-01-tasks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/03_supervised_classification/03-01-tasks/</guid>
      <description>&lt;p&gt;In classification, the task is to predict a categorical (binary or multi-class) label. In this section, we illustrate the concept of classification with some typical examples.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 3.2: Basic Definitions</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/03_supervised_classification/03-02-classification-basicdefs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/03_supervised_classification/03-02-classification-basicdefs/</guid>
      <description>&lt;p&gt;Although we are primarily interested in actual class labels, classification models usually output scores or probabilities first. We will explain why, introduce the concepts of decision regions and decision boundaries, and finally discern two fundamental approaches to constructing classifiers: the generative approach and the discriminant approach.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 3.3: Linear Classifiers</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/03_supervised_classification/03-03-classification-linear/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/03_supervised_classification/03-03-classification-linear/</guid>
      <description>&lt;p&gt;Linear classifiers are an essential subclass of classification models. This section provides the definition of a linear classifier and depicts differences between linear and non-linear decision boundaries.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 3.4: Logistic Regression</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/03_supervised_classification/03-04-classification-logistic/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/03_supervised_classification/03-04-classification-logistic/</guid>
      <description>&lt;p&gt;Logistic regression is a discriminant approach toward constructing a classifier. We will motivate logistic regression via the logistic function, define the log-loss for optimization and illustrate the approach in 1D and 2D.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 3.5: Discriminant Analysis</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/03_supervised_classification/03-05-classification-discranalysis/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/03_supervised_classification/03-05-classification-discranalysis/</guid>
      <description>&lt;p&gt;Discriminant analysis is a generative approach toward constructing a classifier. We distinguish between linear (LDA) and quadratic (QDA) discriminant analysis, where the latter is a more flexible approach subsuming the first.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 4.1: Introduction</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-01-intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-01-intro/</guid>
      <description>&lt;p&gt;It is a crucial part of machine learning to evaluate the performance of a learner. We will explain the concept of generalization error and the difference between inner and outer loss.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 4.2: Measures Regression</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-02-measures-regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-02-measures-regression/</guid>
      <description>&lt;p&gt;In this section we familiarize ourselves with essential performance measures for regression. In particular, mean squared error (MSE), mean absolute error (MAE), and a straightforward generalization of $R^2$ are discussed.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 4.3: Measures Classification</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-03-measures-classification/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-03-measures-classification/</guid>
      <description>&lt;p&gt;Analogous to regression, we consider essential performance measures for classification. As a classifier predicts either class labels or scores/probabilities, its performance can be evaluated based on these two notions. We show some performance measures for classification, including misclassification error rate (MCE), accuracy (ACC) and Brier score (BS). In addition, we will see confusion matrices and learn about costs.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 4.4: Measures Classification ROC</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-04-measures-classification-roc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-04-measures-classification-roc/</guid>
      <description>&lt;p&gt;From the confusion matrix we can calculate a variety of &amp;ldquo;ROC&amp;rdquo; metrics. Among others, we will explain true positive rate, negative predictive value and the $F1$ measure.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 4.5: Measures Classification ROC Visualization</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-05-measures-classification-roc-space/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-05-measures-classification-roc-space/</guid>
      <description>&lt;p&gt;In this section, we explain the ROC curve and how to calculate it. In addition, we will present AUC and partial AUC as global performance measures.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 4.6: Overfitting</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-06-overfitting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-06-overfitting/</guid>
      <description>&lt;p&gt;When a machine learning model performs well on training data but doesn&amp;rsquo;t generalize on the test data, we speak of overfitting. We will show you examples of this behavior and how to diagnose overfitting.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 4.7: Training Error</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-07-train/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-07-train/</guid>
      <description>&lt;p&gt;There are two types of errors: training error and test error. The focus of this section is on the training error and related difficulties.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 4.8: Test Error</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-08-test/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-08-test/</guid>
      <description>&lt;p&gt;While we can infer some information about the learning process from training errors (e.g., the state of iterative optimization), we are truly interested in generalization ability, and thus in the test error on previously unseen data.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 4.9: Resampling</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-09-resampling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-09-resampling/</guid>
      <description>&lt;p&gt;Different resampling techniques help to assess the performance of a learner. We will introduce cross-validation (with and without stratification), bootstrap and subsampling.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter Chapter 3.6: Naive Bayes</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/03_supervised_classification/03-06-classification-naivebayes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/03_supervised_classification/03-06-classification-naivebayes/</guid>
      <description>&lt;p&gt;Naive Bayes is a generative approach based on an assumption of conditional independence and closely related to LDA and QDA.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Cheat Sheets</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/appendix/01_cheat_sheets/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/appendix/01_cheat_sheets/</guid>
      <description> I2ML :: BASICS Download  </description>
    </item>
    
    <item>
      <title>Errata</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/appendix/02_errata/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/appendix/02_errata/</guid>
      <description>Errata in the slides shown in the videos  Chapter 1.4 (Models &amp;amp; Parameters) - slide 5/10: d-dimensional vector, not p-dimensional Chapter 4.3 (Simple Measures for Classification) - slide 6/9: Error in cost matrix Chapter 5.2 (CART: Splitting Criteria) - slide 12/12: Error in result of Gini Chapter 6.2 (Forests: Intro) - slides 7/8 and 8/8: Error in OOB error Chapter 6.4 (Forests: Feature importance) - slide 3/3: Error in permutation based variable importance  </description>
    </item>
    
  </channel>
</rss>
