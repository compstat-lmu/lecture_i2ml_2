<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Introduction to Machine Learning (I2ML)</title>
    <link>https://compstat-lmu.github.io/lecture_i2ml_2/</link>
    <description>Recent content on Introduction to Machine Learning (I2ML)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://compstat-lmu.github.io/lecture_i2ml_2/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/team/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/team/</guid>
      <description>Team  Bernd Bischl wrote the initial version of most of the course material, and teaches various master courses in ML and DL at the LMU for stats and data science. Ludwig Bothmann joined the team in summer 2020 and is in charge of the classes held in winter 2021/22 at LMU Munich. Fabian Scheipl joined the team in fall 2018 and contributed to the slides, videos and code demos. Heidi Seibold joined the team in fall 2019 and is in charge of the classes held in spring 2020 at LMU Munich and University of Bielefeld.</description>
    </item>
    
    <item>
      <title>Chapter 1.1: What is ML?</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/01_ml_basics/01-01-basics-whatisml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/01_ml_basics/01-01-basics-whatisml/</guid>
      <description>&lt;p&gt;A subtopic of artificial intelligence, machine learning is a mathematically well-defined discipline and usually constructs predictive or decision models from data rather than hardwiring them. In this section, you will see some typical examples of where machine learning is applied and the main directions of the field.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 1.2: Data</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/01_ml_basics/01-02-data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/01_ml_basics/01-02-data/</guid>
      <description>&lt;p&gt;In this section we explain the basic structure of tabular data used in machine learning. We will discern targets from features, talk about labeled and unlabeled data and introduce the concept of the data-generating process.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 1.3: Tasks</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/01_ml_basics/01-03-tasks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/01_ml_basics/01-03-tasks/</guid>
      <description>&lt;p&gt;The tasks of supervised learning can roughly be divided in two categories: regression (for continuous outcome) and classification (for categorical outcome). We will present some examples.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 1.4: Models and Parameters</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/01_ml_basics/01-04-models-parameters/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/01_ml_basics/01-04-models-parameters/</guid>
      <description>&lt;p&gt;We introduce models as functional hypotheses about the mapping from feature to target space that allow us to make predictions by computing a function of the input data. Frequently in machine learning, models are understood to be parameterized curves, which is illustrated by several examples.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 1.5: Learner</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/01_ml_basics/01-05-learner/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/01_ml_basics/01-05-learner/</guid>
      <description>&lt;p&gt;Roughly speaking, learners (endowed with a specific hyperparameter configuration) take training data and return a model.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 1.6: Losses and Risk Minimization</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/01_ml_basics/01-06-riskminimization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/01_ml_basics/01-06-riskminimization/</guid>
      <description>&lt;p&gt;In order to find good solutions we need a concept to evaluate and compare models. To this end, the concepts of &lt;em&gt;loss function&lt;/em&gt;, &lt;em&gt;risk&lt;/em&gt; and &lt;em&gt;empirical risk minimization&lt;/em&gt; are introduced.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 1.7: Optimization</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/01_ml_basics/01-07-optimization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/01_ml_basics/01-07-optimization/</guid>
      <description>&lt;p&gt;In this section we study parameter optimization as computational solution to machine learning problems. We address pitfalls in non-convex optimization problems and introduce the fundamental concept of gradient descent.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 1.8: Components of a Learner</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/01_ml_basics/01-08-learnercomponents-hro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/01_ml_basics/01-08-learnercomponents-hro/</guid>
      <description>&lt;p&gt;Nearly all supervised learning algorithms can be described in terms of three components: 1) hypothesis space, 2) risk, and 3) optimization. In this section, we explain how these components interact and why this is a very useful concept for many supervised learning approaches.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 10.1: Intro to mlr3</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/10_mlr3/10-01-intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/10_mlr3/10-01-intro/</guid>
      <description>&lt;p&gt;In this section, we introduce the basic concepts of the R package mlr3.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 10.2: Resampling with mlr3</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/10_mlr3/10-02-resampling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/10_mlr3/10-02-resampling/</guid>
      <description>&lt;p&gt;mlr3 supports various forms of resampling, which we will demonstrate in this section.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 10.3: Tuning with mlr3</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/10_mlr3/10-03-tuning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/10_mlr3/10-03-tuning/</guid>
      <description>&lt;p&gt;We can easily conduct hyperparameter tuning with mrl3&amp;rsquo;s modular ecosystem, defining custom search spaces and suitable tuning algorithms.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 10.4: Pipelines with mlr3</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/10_mlr3/10-04-pipelines/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/10_mlr3/10-04-pipelines/</guid>
      <description>&lt;p&gt;Pipelines provide an important tool to package all relevant machine learning step into a single object that can be treated just like any other learner, ensuring adherence to the train-test split principle even in the face of complex analyses.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 11.10: Maximum Likelihood Estimation vs Empirical Risk Minimization I</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/11_advriskmin/11-10-max-likelihood-l2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/11_advriskmin/11-10-max-likelihood-l2/</guid>
      <description>&lt;p&gt;We discuss the connection between maximum likelihood estimation and risk minimization. We discuss the correspondence between a Gaussian error distribution and \(L2\) loss.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 11.11: Maximum Likelihood Estimation vs Empirical Risk Minimization II</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/11_advriskmin/11-11-max-likelihood-other/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/11_advriskmin/11-11-max-likelihood-other/</guid>
      <description>&lt;p&gt;We discuss the connection between maximum likelihood estimation and risk minimization for further losses (\(L1\) loss, Bernoulli loss).&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 11.1: Risk Minimizers</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/11_advriskmin/11-01-risk-minimizer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/11_advriskmin/11-01-risk-minimizer/</guid>
      <description>&lt;p&gt;We introduce important concepts in theoretical risk minimization: risk minimizer, Bayes risk, Bayes regret, consistent learners and the optimal constant model.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 11.2: Pseudo-Residuals</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/11_advriskmin/11-02-pseudo-residuals/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/11_advriskmin/11-02-pseudo-residuals/</guid>
      <description>&lt;p&gt;We introduce the concept of pseudo-residuals, i.e., loss residuals in function space, and discuss their relation to gradient descent.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 11.3: L2 Loss</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/11_advriskmin/11-03-regression-l2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/11_advriskmin/11-03-regression-l2/</guid>
      <description>&lt;p&gt;In this section, we revisit \(L2\) loss and derive its risk minimizer &amp;ndash; the conditional mean &amp;ndash; and optimal constant model &amp;ndash; the empirical mean of observed target values.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 11.4: L1 Loss</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/11_advriskmin/11-04-regression-l1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/11_advriskmin/11-04-regression-l1/</guid>
      <description>&lt;p&gt;In this section, we revisit \(L1\) loss and derive its risk minimizer &amp;ndash; the conditional median &amp;ndash; and optimal constant model &amp;ndash; the empirical median of observed target values.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 11.5: Advanced Regression Losses</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/11_advriskmin/11-05-regression-further-losses/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/11_advriskmin/11-05-regression-further-losses/</guid>
      <description>&lt;p&gt;In this section, we introduce and discuss the following advanced regression losses: Huber, log-cosh, Cauchy, log-barrier, epsilon-insensitive, and quantile loss.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 11.6: 0-1 Loss</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/11_advriskmin/11-06-classification-01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/11_advriskmin/11-06-classification-01/</guid>
      <description>&lt;p&gt;In this section, we revisit the 0-1 loss and derive its risk minimizer .&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 11.7: Bernoulli Loss</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/11_advriskmin/11-07-classification-bernoulli/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/11_advriskmin/11-07-classification-bernoulli/</guid>
      <description>&lt;p&gt;We study the Bernoulli loss and derive its risk minimizer and optimal constant model. We further discuss the connection between Bernoulli loss minimization and tree splitting according to the entropy criterion.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 11.8: Brier Score</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/11_advriskmin/11-08-classification-brier/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/11_advriskmin/11-08-classification-brier/</guid>
      <description>&lt;p&gt;In this section, we introduce the Brier score and derive its risk minimizer and optimal constant model. We further discuss the connection between Brier score minimization and tree splitting according to the Gini index.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 11.9: Advanced Classification Losses</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/11_advriskmin/11-09-classification-further-losses/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/11_advriskmin/11-09-classification-further-losses/</guid>
      <description>&lt;p&gt;In this section, we introduce and discuss the following advanced classification losses: (squared) hinge loss, \(L2\) loss on scores, exponential loss, and AUC loss.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 12.1: Multiclass Classification and Losses</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/12_multiclass/12-01-losses/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/12_multiclass/12-01-losses/</guid>
      <description>&lt;p&gt;In this section, we introduce the basic concepts in multiclass (MC) classification and important MC losses: MC 0-1 loss, MC brier score, and MC logarithmic loss.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 12.2: Softmax Regression</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/12_multiclass/12-02-softmax-regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/12_multiclass/12-02-softmax-regression/</guid>
      <description>&lt;p&gt;In this section, we introduce softmax regression as a generalization of logistic regression.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 12.3: One-vs-One and One-vs-Rest</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/12_multiclass/12-03-binary-reduction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/12_multiclass/12-03-binary-reduction/</guid>
      <description>&lt;p&gt;It is sometimes advisable to address a multiclass problem as a set of binary ones. We discuss two ways to reduce a multiclass problem to multiple binary classification problems: one-vs-one and one-vs-rest.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 12.4: Designing Codebooks and ECOC</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/12_multiclass/12-04-codebooks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/12_multiclass/12-04-codebooks/</guid>
      <description>&lt;p&gt;In this section, we introduce codebooks as a general concept for multiclass to binary reduction.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 13.1: Entropy</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/13_information_theory/13-01-entropy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/13_information_theory/13-01-entropy/</guid>
      <description>&lt;p&gt;We introduce entropy, which expresses the expected information for discrete random variables, as a central concept in information theory.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 13.2: Differential Entropy</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/13_information_theory/13-02-diffent/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/13_information_theory/13-02-diffent/</guid>
      <description>&lt;p&gt;In this section, we extend the definition of entropy to the continuous case.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 13.3: Kullback-Leibler Divergence</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/13_information_theory/13-03-kl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/13_information_theory/13-03-kl/</guid>
      <description>&lt;p&gt;The Kullback-Leibler divergence (KL) is an important quantity for measuring the difference between two probability distributions. We discuss different intuitions for KL and relate it to risk minimization and likelihood ratios.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 13.4: Entropy and Optimal Code Length</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/13_information_theory/13-04-sourcecoding/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/13_information_theory/13-04-sourcecoding/</guid>
      <description>&lt;p&gt;In this section, we introduce source coding and discuss how entropy can be understood as optimal code length.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 13.5: Cross-Entropy, KL and Source Coding</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/13_information_theory/13-05-cross-entropy-kld/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/13_information_theory/13-05-cross-entropy-kld/</guid>
      <description>&lt;p&gt;We introduce cross-entropy as a further information-theoretic concept and discuss the connection between entropy, cross-entropy, and Kullback-Leibler divergence.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 13.6: Information Theory for Machine Learning</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/13_information_theory/13-06-ml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/13_information_theory/13-06-ml/</guid>
      <description>&lt;p&gt;In this section, we discuss how information-theoretic concepts are used in machine learning and demonstrate the equivalence of KL minimization and maximum likelihood maximization, as well as how (cross-)entropy can be used as a loss function.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 13.7: Joint Entropy and Mutual Information</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/13_information_theory/13-07-mutual-info/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/13_information_theory/13-07-mutual-info/</guid>
      <description>&lt;p&gt;Information theory also provides means of quantifying relations between two random variables that extend the concept of (linear) correlation. We discuss joint entropy, conditional entropy, and mutual information.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 14.1: Curse of Dimensionality</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/14_cod/14-01-cod/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/14_cod/14-01-cod/</guid>
      <description>&lt;p&gt;In this section, we discuss why our geometric intuition fails in high-dimensional spaces and introduce the phenomenon of the curse of dimensionality.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 14.2: Curse of Dimensionality - Examples</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/14_cod/14-02-cod-examples/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/14_cod/14-02-cod-examples/</guid>
      <description>&lt;p&gt;In this section, we show examples of how \((k\)-NN and the linear model suffer from the the curse of dimensionality.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 15.1: Examples of Hypothesis Spaces</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/15_hypospaces_capacity/15-01-hypospaces-examples/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/15_hypospaces_capacity/15-01-hypospaces-examples/</guid>
      <description>&lt;p&gt;In this section, we show examples for formal definitions of hypothesis spaces.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 15.2: Capacity and Overfitting</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/15_hypospaces_capacity/15-02-hypospaces-capacity-overfitting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/15_hypospaces_capacity/15-02-hypospaces-capacity-overfitting/</guid>
      <description>&lt;p&gt;In this section, we discuss how the capacity of a hypothesis influences the overfitting behavior of a learner.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 15.3: PAC Learning and VC Dimension</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/15_hypospaces_capacity/15-03-complexity/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/15_hypospaces_capacity/15-03-complexity/</guid>
      <description>&lt;p&gt;The &amp;ldquo;no free lunch&amp;rdquo; theorem implies that we cannot construct a universally optimal learner. More precisely, only finite hypothesis spaces, as measured by Vapnik-Chervonenkis (VC) dimension, are agnostic probably-approximately-correct (PAC) learnable, which we will discuss in this section.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 15.4: Bias-Variance Decomposition</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/15_hypospaces_capacity/15-04-bias-variance-decomposition/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/15_hypospaces_capacity/15-04-bias-variance-decomposition/</guid>
      <description>&lt;p&gt;In this section, we derive and discuss the classic bias-variance decomposition of the generalization error of an inducer.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 16.10: Early Stopping</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/16_regularization/16-10-early-stopping/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/16_regularization/16-10-early-stopping/</guid>
      <description>&lt;p&gt;In this section, we introduce early stopping and show how it can act as a regularizer.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 16.1: Introduction to Regularization</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/16_regularization/16-01-regu-intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/16_regularization/16-01-regu-intro/</guid>
      <description>&lt;p&gt;In this section, we revisit overfitting and introduce regularization as a remedy.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 16.2: Lasso and Ridge Regression</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/16_regularization/16-02-l1l2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/16_regularization/16-02-l1l2/</guid>
      <description>&lt;p&gt;We introduce Lasso and Ridge regression as the key approaches to regularizing linear models.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 16.3: Lasso vs Ridge Regression</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/16_regularization/16-03-l1vsl12/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/16_regularization/16-03-l1vsl12/</guid>
      <description>&lt;p&gt;This section provides a detailed comparison between Lasso and Ridge regression.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 16.4: Elastic Net and Regularization for GLMs</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/16_regularization/16-04-enetlogreg/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/16_regularization/16-04-enetlogreg/</guid>
      <description>&lt;p&gt;In this section, we introduce the elastic net as a combination of Ridge and Lasso regression and discuss regularization for logistic regression.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 16.5: Regularization for Underdetermined Problems</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/16_regularization/16-05-underdetermined/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/16_regularization/16-05-underdetermined/</guid>
      <description>&lt;p&gt;In this section, we discuss how regularization can make ill-posed problems well-defined.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 16.6: L0 Regularization</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/16_regularization/16-06-l0/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/16_regularization/16-06-l0/</guid>
      <description>&lt;p&gt;In this section, we introduce \(LQ\) regularization and particularly discuss \(L0\) regularization as an important special case besides \(L1\) and \(L2\) that penalizes the number of non-zero parameters.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 16.7: Regularization in NonLinear Models and Bayesian Priors</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/16_regularization/16-07-nonlin-bayes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/16_regularization/16-07-nonlin-bayes/</guid>
      <description>&lt;p&gt;In this section, we motivate regularization from a Bayesian perspective, showing how different penalty terms correspond to different Bayesian priors.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 16.8: Geometric Analysis of L2 Regularization and Weight Decay</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/16_regularization/16-08-geom-l2-wdecay/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/16_regularization/16-08-geom-l2-wdecay/</guid>
      <description>&lt;p&gt;In this section, we provide a geometric understanding of \(L2\) regularization, showing how parameters are shrunk according to the eigenvalues of the Hessian of empirical risk, and discuss its correspondence to weight decay.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 16.9: Geometric Analysis of L1 Regularization</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/16_regularization/16-09-geom-l1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/16_regularization/16-09-geom-l1/</guid>
      <description>&lt;p&gt;In this section, we provide a geometric understanding of \(L2\) regularization and show that it encourages sparsity in the parameter vector.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 17.1: Linear Hard Margin SVM</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/17_linear_svm/17-01-hard-margin/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/17_linear_svm/17-01-hard-margin/</guid>
      <description>&lt;p&gt;Hard margin SVMs seek perfect data separation. We introduce the linear hard margin SVM problem as a quadratic optimization program.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 17.2: Hard Margin SVM Dual</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/17_linear_svm/17-02-hard-margin-dual/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/17_linear_svm/17-02-hard-margin-dual/</guid>
      <description>&lt;p&gt;In this section, we derive the dual variant of the linear hard-margin SVM problem, a computationally favorable formulation.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 17.3: Soft Margin SVM</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/17_linear_svm/17-03-soft-margin/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/17_linear_svm/17-03-soft-margin/</guid>
      <description>&lt;p&gt;Hard margin SVMs are often not applicable to practical questions because they fail when the data are not linearly separable. Moreover, for the sake of generalization, we will often accept some violations to keep the margin large enough for reliable class separation. Therefore, we introduce the soft margin linear SVM.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 17.4: SVMs and Empirical Risk Minimization</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/17_linear_svm/17-04-erm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/17_linear_svm/17-04-erm/</guid>
      <description>&lt;p&gt;In this section, we show how the SVM problem can be understood as an instance of empirical risk minimization.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 17.5: SVM Training</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/17_linear_svm/17-05-optimization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/17_linear_svm/17-05-optimization/</guid>
      <description>&lt;p&gt;The linear SVM problem is challenging due to its non-differentiability. In this section, we present methods of optimization.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 18.1: Feature Generation for Nonlinear Separation</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/18_nonlinear_svm/18-01-featuregen/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/18_nonlinear_svm/18-01-featuregen/</guid>
      <description>&lt;p&gt;We show how nonlinear feature maps project the input data to transformed spaces where they become linearly separable.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 18.2: The Kernel Trick</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/18_nonlinear_svm/18-02-kernel-trick/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/18_nonlinear_svm/18-02-kernel-trick/</guid>
      <description>&lt;p&gt;In this section, we show how nonlinear SVMs work their magic by introducing nonlinearity efficiently via the kernel trick.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 18.3: The Polynomial Kernel</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/18_nonlinear_svm/18-03-kernel-poly/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/18_nonlinear_svm/18-03-kernel-poly/</guid>
      <description>&lt;p&gt;In this section, we introduce the polynomial kernel in the context of SVMs and demonstrate how different polynomial degrees affect decision boundaries.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 18.4: Reproducing Kernel Hilbert Space and Representer Theorem</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/18_nonlinear_svm/18-04-rkhs-repr/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/18_nonlinear_svm/18-04-rkhs-repr/</guid>
      <description>&lt;p&gt;In this section, we introduce important theoretical background on nonlinear SVMs that essentially allows us to express them as a weighted sum of basis functions.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 18.5: The Gaussian RBF Kernel</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/18_nonlinear_svm/18-05-kernel-rbf/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/18_nonlinear_svm/18-05-kernel-rbf/</guid>
      <description>&lt;p&gt;In this section, we introduce the popular Gaussian RBF kernel and discuss its properties.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 18.6: SVM Model Selection</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/18_nonlinear_svm/18-06-model-sel/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/18_nonlinear_svm/18-06-model-sel/</guid>
      <description>&lt;p&gt;In this section, we discuss the importance of SVM hyperparameters for adequate solutions.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 19.1: The Bayesian Linear Model</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/19_gaussian_processes/19-01-bayes-lm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/19_gaussian_processes/19-01-bayes-lm/</guid>
      <description>&lt;p&gt;We begin by reviewing the Bayesian formulation of a linear model and show that instead of point estimates for parameters and predictions, we obtain an entire posterior and predictive distribution.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 19.2: Gaussian Processes</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/19_gaussian_processes/19-02-basic/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/19_gaussian_processes/19-02-basic/</guid>
      <description>&lt;p&gt;In this section, we introduce the basic idea behind Gaussian processes. We move from weight to function space and build some intuition on distributions over functions, discuss GPs&#39; marginalization property, derive GP priors, and interpret GPs as indexed families.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 19.3: Covariance Functions for GPs</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/19_gaussian_processes/19-03-covariance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/19_gaussian_processes/19-03-covariance/</guid>
      <description>&lt;p&gt;In this section, we discuss the role of covariance functions in GPs and introduce the most common choices.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 19.4: Gaussian Process Prediction</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/19_gaussian_processes/19-04-prediction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/19_gaussian_processes/19-04-prediction/</guid>
      <description>&lt;p&gt;In this section, we show how to derive the posterior process and discuss further properties of GPs as well as noisy GPs.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 19.5: Gaussian Process Training</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/19_gaussian_processes/19-05-training/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/19_gaussian_processes/19-05-training/</guid>
      <description>&lt;p&gt;In this section, we show how Gaussian processes are actually trained using maximum likelihood estimation and exploiting the fact that we can learn covariance functions&#39; hyperparameters on the fly.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 2.1: Loss Functions for Regression</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/02_supervised_regression/02-01-losses/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/02_supervised_regression/02-01-losses/</guid>
      <description>&lt;p&gt;\(L1\) and \(L2\) are two essential loss functions used for evaluating the performance of regression models. This section defines \(L1\) and \(L2\) loss and explains the differences.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 2.2: Linear Regression Models</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/02_supervised_regression/02-02-linearmodel/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/02_supervised_regression/02-02-linearmodel/</guid>
      <description>&lt;p&gt;In this section, we explain how the linear regression model can be used from a machine learning perspective to predict a continuous numerical target variable. We use the concepts of loss function and empirical risk minimization to find the linear model that best fits the data.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 2.3: Polynomial Regression Models</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/02_supervised_regression/02-03-polynomials/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/02_supervised_regression/02-03-polynomials/</guid>
      <description>&lt;p&gt;This section introduces polynomials to obtain more flexible models for the regression task. We explain the connection to the basic linear model and discuss the problem of overfitting.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 20.1: Introduction to Boosting / AdaBoost</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/20_boosting/20-01-intro-adaboost/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/20_boosting/20-01-intro-adaboost/</guid>
      <description>&lt;p&gt;In this section, we introduce the pioneering AdaBoost algorithm.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 20.2: Boosting Concept</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/20_boosting/20-02-gradient-boosting-concept/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/20_boosting/20-02-gradient-boosting-concept/</guid>
      <description>&lt;p&gt;In this section, we discuss the general boosting principle: performing gradient descent in function space by repeatedly fitting new base learner components to the current pseudo-residuals.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 3.1: Classification Tasks</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/03_supervised_classification/03-01-tasks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/03_supervised_classification/03-01-tasks/</guid>
      <description>&lt;p&gt;In classification, the task is to predict a categorical (binary or multi-class) label. In this section, we illustrate the concept of classification with some typical examples.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 3.2: Basic Definitions</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/03_supervised_classification/03-02-classification-basicdefs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/03_supervised_classification/03-02-classification-basicdefs/</guid>
      <description>&lt;p&gt;Although we are primarily interested in actual class labels, classification models usually output scores or probabilities first. We will explain why, introduce the concepts of decision regions and decision boundaries, and finally discern two fundamental approaches to constructing classifiers: the generative approach and the discriminant approach.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 3.3: Linear Classifiers</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/03_supervised_classification/03-03-classification-linear/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/03_supervised_classification/03-03-classification-linear/</guid>
      <description>&lt;p&gt;Linear classifiers are an essential subclass of classification models. This section provides the definition of a linear classifier and depicts differences between linear and non-linear decision boundaries.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 3.4: Logistic Regression</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/03_supervised_classification/03-04-classification-logistic/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/03_supervised_classification/03-04-classification-logistic/</guid>
      <description>&lt;p&gt;Logistic regression is a discriminant approach toward constructing a classifier. We will motivate logistic regression via the logistic function, define the log-loss for optimization and illustrate the approach in 1D and 2D.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 3.5: Discriminant Analysis</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/03_supervised_classification/03-05-classification-discranalysis/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/03_supervised_classification/03-05-classification-discranalysis/</guid>
      <description>&lt;p&gt;Discriminant analysis is a generative approach toward constructing a classifier. We distinguish between linear (LDA) and quadratic (QDA) discriminant analysis, where the latter is a more flexible approach subsuming the first.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 3.6: Naive Bayes</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/03_supervised_classification/03-06-classification-naivebayes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/03_supervised_classification/03-06-classification-naivebayes/</guid>
      <description>&lt;p&gt;Naive Bayes is a generative approach based on an assumption of conditional independence and closely related to LDA and QDA.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 4.1: Introduction</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-01-intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-01-intro/</guid>
      <description>&lt;p&gt;It is a crucial part of machine learning to evaluate the performance of a learner. We will explain the concept of generalization error and the difference between inner and outer loss.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 4.2: Measures Regression</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-02-measures-regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-02-measures-regression/</guid>
      <description>&lt;p&gt;In this section we familiarize ourselves with essential performance measures for regression. In particular, mean squared error (MSE), mean absolute error (MAE), and a straightforward generalization of $R^2$ are discussed.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 4.3: Measures Classification</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-03-measures-classification/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-03-measures-classification/</guid>
      <description>&lt;p&gt;Analogous to regression, we consider essential performance measures for classification. As a classifier predicts either class labels or scores/probabilities, its performance can be evaluated based on these two notions. We show some performance measures for classification, including misclassification error rate (MCE), accuracy (ACC) and Brier score (BS). In addition, we will see confusion matrices and learn about costs.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 4.4: Measures Classification ROC</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-04-measures-classification-roc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-04-measures-classification-roc/</guid>
      <description>&lt;p&gt;From the confusion matrix we can calculate a variety of &amp;ldquo;ROC&amp;rdquo; metrics. Among others, we will explain true positive rate, negative predictive value and the $F1$ measure.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 4.5: Measures Classification ROC Visualization</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-05-measures-classification-roc-space/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-05-measures-classification-roc-space/</guid>
      <description>&lt;p&gt;In this section, we explain the ROC curve and how to calculate it. In addition, we will present AUC and partial AUC as global performance measures.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 4.6: Overfitting</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-06-overfitting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-06-overfitting/</guid>
      <description>&lt;p&gt;When a machine learning model performs well on training data but doesn&amp;rsquo;t generalize on the test data, we speak of overfitting. We will show you examples of this behavior and how to diagnose overfitting.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 4.7: Training Error</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-07-train/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-07-train/</guid>
      <description>&lt;p&gt;There are two types of errors: training error and test error. The focus of this section is on the training error and related difficulties.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 4.8: Test Error</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-08-test/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-08-test/</guid>
      <description>&lt;p&gt;While we can infer some information about the learning process from training errors (e.g., the state of iterative optimization), we are truly interested in generalization ability, and thus in the test error on previously unseen data.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 4.9: Resampling</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-09-resampling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/04_evaluation/04-09-resampling/</guid>
      <description>&lt;p&gt;Different resampling techniques help to assess the performance of a learner. We will introduce cross-validation (with and without stratification), bootstrap and subsampling.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 5.1: k-Nearest Neighbors (k-NN)</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/05_knn/05-01-knn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/05_knn/05-01-knn/</guid>
      <description>&lt;p&gt;We demonstrate that distances in feature space are crucial in \(k\)-NN regression / classification and show how we can form predictions by averaging / majority vote. In this, \(k\)-NN is a very local model and works without distributional assumptions.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 6.1: Introduction</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/06_trees/06-01-intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/06_trees/06-01-intro/</guid>
      <description>&lt;p&gt;Decision trees are an important type of machine learning models and come in two main types: classification and regression trees. In this section, we explain the general idea of CART and show how they recursively divide up the input space into ever smaller rectangular partitions.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 6.2: Splitting Criteria</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/06_trees/06-02-splitcriteria/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/06_trees/06-02-splitcriteria/</guid>
      <description>&lt;p&gt;CART algorithms require splitting criteria for trees, which are usually defined in terms of &lt;em&gt;impurity reduction&lt;/em&gt;. In this section we formalize the idea of splitting criteria and explain the details of splitting for both regression and classification.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 6.3: Growing a Tree</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/06_trees/06-03-treegrowing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/06_trees/06-03-treegrowing/</guid>
      <description>&lt;p&gt;In this section, we explain how to grow a tree starting with an empty tree, i.e., a root node containing all the data. It will be shown that trees are grown by recursively applying greedy optimization to each node.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 6.4: Computational Aspects of Finding Splits</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/06_trees/06-04-splitcomputation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/06_trees/06-04-splitcomputation/</guid>
      <description>&lt;p&gt;In this section, we explain the computational aspects of the node-splitting procedure, especially for nominal features. In addition, we illustrate how to deal with missing values.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 6.5: Stopping Criteria &amp; Pruning</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/06_trees/06-05-stoppingpruning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/06_trees/06-05-stoppingpruning/</guid>
      <description>&lt;p&gt;The recursive partitioning procedure used to grow a CART usually leads to problems such as exponential growth of computations, overfitting, and the horizon effect. To deal with these problems, we can use stopping criteria and pruning. In this section, we explain the basis of these two solutions.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 6.6: Discussion</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/06_trees/06-06-discussion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/06_trees/06-06-discussion/</guid>
      <description>&lt;p&gt;In this section we discuss the advantages and disadvantages of CART and mention other tree methodologies.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 7.1: Bagging Ensembles</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/07_forests/07-01-bagging/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/07_forests/07-01-bagging/</guid>
      <description>&lt;p&gt;Bagging (bootstrap aggregation) is a method for combining many models into a meta-model, which often works much better than its individual components. In this section, we present the basic idea of bagging and explain why and when bagging works.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 7.2: Introduction</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/07_forests/07-02-intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/07_forests/07-02-intro/</guid>
      <description>&lt;p&gt;In this section we investigate random forests, a modification of bagging for trees. We illustrate the effect of ensemble size and show how to compute out-of-bag error estimates.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 7.3: Benchmarking Trees, Forests, and Bagging k-NN</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/07_forests/07-03-benchmark/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/07_forests/07-03-benchmark/</guid>
      <description>&lt;p&gt;We compare the performance of random forests vs. (bagged) CART and (bagged) \(k\)-NN.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 7.4: Feature Importance</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/07_forests/07-04-featureimportance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/07_forests/07-04-featureimportance/</guid>
      <description>&lt;p&gt;In a complex machine learning model, the contributions of the different features to the model performance are difficult to evaluate. The concept of feature importance allows to quantify these effects for random forests.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 7.5: Proximities</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/07_forests/07-05-proximities/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/07_forests/07-05-proximities/</guid>
      <description>&lt;p&gt;The term &lt;em&gt;proximity&lt;/em&gt; refers to the &amp;ldquo;closeness&amp;rdquo; between pairs of cases. Proximities are calculated for each pair of observations and can be derived directly from random forests.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 7.6: Discussion</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/07_forests/07-06-discussion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/07_forests/07-06-discussion/</guid>
      <description>&lt;p&gt;In this section we discuss the advantages and disadvantages of random forests and explain that all advantages of trees also apply here.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 8.1: Introduction</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/08_tuning/08-01-intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/08_tuning/08-01-intro/</guid>
      <description>&lt;p&gt;While model parameters are optimized during training, hyperparameters must be specified before the training. In this section, we will motivate why it is crucial to find good values for, i.e. to &amp;ldquo;tune&amp;rdquo;, these hyperparameters.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 8.2: Problem Definition</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/08_tuning/08-02-tuning-tuningproblem/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/08_tuning/08-02-tuning-tuningproblem/</guid>
      <description>&lt;p&gt;Hyperparameter tuning is the process of finding good model hyperparameters. In this section we formalize the problem of tuning and explain why tuning is computationally hard.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 8.3: Basic Techniques</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/08_tuning/08-03-basicalgos/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/08_tuning/08-03-basicalgos/</guid>
      <description>&lt;p&gt;In this section familiarize ourselves with two simple but popular tuning strategies, namely grid search and random search, and discuss their advantages and disadvantages.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 9.1: Motivation</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/09_nested_resampling/09-01-nestedintro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/09_nested_resampling/09-01-nestedintro/</guid>
      <description>&lt;p&gt;Selecting the best model from a set of potential candidates is an important part of most machine learning problems. By examining an instructive and problematic example, we introduce the untouched-test-set principle.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 9.2: Training - Validation - Testing</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/09_nested_resampling/09-02-trainvalidtest/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/09_nested_resampling/09-02-trainvalidtest/</guid>
      <description>&lt;p&gt;The simplest method to achieve an untouched test set is a 3-way split: the models are first trained on the &lt;em&gt;training set&lt;/em&gt; and then evaluated and compared on the &lt;em&gt;validation set&lt;/em&gt;. After selecting the best model, the final performance will be evaluated on the &lt;em&gt;test set&lt;/em&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 9.3: Nested Resampling</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/09_nested_resampling/09-03-nestedresampling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/chapters/09_nested_resampling/09-03-nestedresampling/</guid>
      <description>&lt;p&gt;In this section, we will explain why and how nested resampling is done.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Cheat Sheets</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/appendix/01_cheat_sheets/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/appendix/01_cheat_sheets/</guid>
      <description> I2ML :: BASICS Download  </description>
    </item>
    
    <item>
      <title>Errata</title>
      <link>https://compstat-lmu.github.io/lecture_i2ml_2/appendix/02_errata/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://compstat-lmu.github.io/lecture_i2ml_2/appendix/02_errata/</guid>
      <description>Errata in the slides shown in the videos  Chapter 1.4 (Models &amp;amp; Parameters) - slide 5/10: d-dimensional vector, not p-dimensional Chapter 4.3 (Simple Measures for Classification) - slide 6/9: Error in cost matrix Chapter 5.2 (CART: Splitting Criteria) - slide 12/12: Error in result of Gini Chapter 6.2 (Forests: Intro) - slides 7/8 and 8/8: Error in OOB error Chapter 6.4 (Forests: Feature importance) - slide 3/3: Error in permutation based variable importance  </description>
    </item>
    
  </channel>
</rss>
